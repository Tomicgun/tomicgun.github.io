---
title: Model Comprehension Version one
author: Thomas
date: 2025-04-17 00:00:00 -5:00
category: Projects
tags: [Open_cv,Clustering,Research,Solo-Project,Computer Vision]
description: This project goal is to take diagrams and find how visual dense the diagram is, using computer vision techniques like open_cv. 
published: true
toc: true
---

# Model Comprehension Version one

## Summary
This project is part of larger research on model comprehension specifically what makes a good UML diagram a 
good UML diagram. In this research we analyzed 700 diagrams for various metrics one of those being model
density. Actually calculating model density by hand would be unreasonable, and thus I suggested we create
a program that can take in these diagram and find the density quickly, reliable and consistently. This program 
uses a variety of techniques this the main ones in this version being the use of a library called open_cv and two
clustering techniques k-means and voronoi tessellation. I spent about 3 weeks researching and finally putting 
together this program and fixing as many bugs and issues I could before I had to go back to school.

## Design and Solution
The broad procedure for what I needed this program to do was find the center points of nodes/boxes
in a UML diagram. Doing this by hand was not hard but time-consuming, and ideally I would want the program to 
display a result like the image bellow and similar to the by hand method. 

![Annotated example of want I wanted to happen](assets/images/DrawnByHandMethod.png)
_This is what I would want the program to display when finding the boxes/nodes_

My plan was to use open_cv's find contour method and then use a voronoi tesselation method. The open_cv method would
find the center points of the boxes and then the program would cluster these center points using voronoi tesselation method.
It would then find the geographic center points of the voronoi regions and calculate the Euclidean distance in pixels.
Then the program would average these distances and find a standard deviation. 

Thus, this would allow us to find the density of digrams in a quick, reliable and consistent way. However, as von Moltke said
a plan never survives contact with the enemy or this case a design never survives implementation.

## Implementation / Main Problems
There were a series of problems often fixing one would cause more, and thus I went down a rabbit hole of whack a bug. 
The main issues where the time limit I had and my lack of knowledge on how to fully take advantage of open_cv and the many methods
and techniques it had to offer. The plan to use open_cv was a good plan but the implementation was much 
harder to do. Since many of the boxes and text where all connected by lines or other elements. 
Open_cv struggled to find the text and boxes and would often simply find many center points nested 
inside each other or in the middle of nowhere. Open_cv in this implementation would prove generally unreliable and
inconsistent. 

![Example photo of bad contour output](assets/images/BadContour.png)
_This is what the find contour got even after fine-tuning_
This problem proved difficult to solve and the main source of my frustration during development. In time and retrospect I 
would solve this problem but not during this version. My other problem was some base assumptions made and lingering issues from open_cv.
Since the open_cv method often produced center points that where ether nonsensical, a huge amount of points or both. I felt it necessary
to cluster it to attempt to filter or dilute these impurities in the data. So I used K-means clustering as I knew how many cluster
I would need and could hopefully get better results. However, my choice here was not perfect as K-means is not consistent, and
often the resulting cluster could be quite different even run to run.

![Example of k-mean clustering on the diagrams](assets/images/Cluster.png)
_This is a base diagram and its corresponding k-means cluster  with the triangles being center points on the cluster graph_


I would then take the center points from the k-means graph and then cluster it again in a voronoi tesselation method. 
Again to filter out bad data and attempt to find the broad regions of a diagram. These final center points where then used to 
calculate the final average distance and standard deviation. However, a new issue arose that the voronoi tesselation would 
produce terrible results often returning graphs and data with only a single point or worse no points. This issue 
illuded me, and was the most troubling as I agonized over it for days before finally having to admit defeat and end my work 
on this program due to me running out of time.

![Example of bad voronoi graph](assets/images/VoronoiMerged1.png)
_A example of a bad voronoi outputs with just one point no points, this occurred 154 times_

Overall this program suffered from many issues with the biggest being open_cv unreliability in finding nodes and boxes of our diagrams.
Then the voronoi tesselation method often failing or producing terrible data with a staggering 22% of all 700 diagrams failing to produce 
adequate or usable data. This means 154 of our diagrams either had bad data or no data at all mostly due to this single method. 
Also, the fact that this fault illuded me, and me ultimately running out of time was frustrating.

## Step-by-step process
1. For every pdf file, create a png
2. The program then converts the png to grayscale applies and a gaussian blur
3. The program then applies an opencv invert binary threshold
4. The program then applies an open cv method for finding the contours of the png and finds the center points of these contours.
5. The program then uses the center points in a K-means cluster algorithm to reduce the number of center points 
and find new center points of the clusters.
6. The program then uses the k-means center points to create a Voronoi tessellation method.
7. The program then finds the center points of the Voronoi tessellation clusters
8. The program then uses the Voronoi center points and calculates the average distance and standard deviation of the average distance.

## Conclusion 

This project was a seris of compromises and best solution's given the strict time limit. We had some usable data and
a good starting point for further development both in the research and for the program. This program has serious faults especially 
with the open_cv methods and the voronoi tesselation method I used. These problems where not insurmountable and given more time could
be solved. There were also a myriad of minor bug fixes, readable issues and general lack of documentation that would be needed for the future.
This program would be stuck in this state for about three months till in my final semester of college I decided to once again take up the
mantel and see if I could not fix this program and make it want I always wanted it to be. 

> If you want to see how I fixed this program and what new features I added to this 
> [go to Model Comprehension Version 2](https://tomicgun.github.io/posts/MCV2/)
{: .prompt-info}
