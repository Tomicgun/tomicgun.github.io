---
title: Model Comprehension Version two
author: Thomas
date: 2025-04-17 00:00:00 -5:00
category: Projects
tags: []
description: This project goal is to take diagrams and find how visual dense the diagram is, using computer vision techniques like OCR and open_cv methods.
published: false
pin: true
toc: true
---

# Model Comprehension Version 2

> This project relates to its [predecessor Model Comprehension V1](https://tomicgun.github.io/posts/MCV1/) in that post I cover alot of the background information and purpose of this project.
> and cover the background and reason why I created this project.
 {: .prompt-info}
>

## Summary
This project was initiated in the summer of last year with the objective to determine what makes a UML diagram a good UML diagram. 
This simple questions had many complex facets to it. One question was whether a more dense or less dense diagram was better. 
Thus this algorithm was made to automatically find the density of UML diagrams based of average pixel distance between significant semantic 
and diagrammatic points on the diagram or points of interest (POI). 

The initial version of the software employed basic computer vision techniques 
and image processing methods to automatically determine diagram density. However, this approach proved unreliable, failing to identify meaningful information in exactly 22% of cases. 
This error rate was deemed unacceptable, prompting a redesign of the algorithm to enhance its ability to detect textual and diagrammatic elements. 
The revised method also incorporates more advanced clustering and voronoi techniques to improve analysis and increase accuracy, reduce errors and failure rates.


## Project Rationale
When I finished Model Comprehension version 1, I had some issues that where left unsolved. 
The main issues was the algorithm 22% of the time simply did not work or output data that was
clearly wrong. Also, the voronoi generated from the custom voronoi method where not good and felt like
it was throwing out data when it should not be. This and many other minor issues found in a inception 
session done on the project finally convicted me to actually go back and fix the algorithm.
I had four main goals in this rework.

1. Fix the Voronoi Method and the open_cv method
2. Refactor and Reformat the code for it to be more readable and maintainable
3. Provided Proper Documentation of All the methods and Algorithm in general.

These issue will be fixed, and along the way I will change some features, and add some new features
as I would up having some extra time and decided to add some bells and whistle to the project.

## Solution

## Tech Stack

## Main Problem

## Step-by-step process
1. For every pdf file it creates a png file
2. For every png file it then runs a optical character recognition (OCR)  method
3. If the OCR returns less than 6 POIs it then runs a open cv method
4. The algorithm then uses the method that returns the most POI’s.
5. If the number of POI’s is larger than 100 it will use mean shift clustering to reduce the number of POI’s
6. The data from the above steps is used in a custom voronoi method to find voronoi center points. 
7. These voronoi center points are used to calculate average distance, and standard deviation of the distances.

## Demo Video

## Conclusion 






